{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Save and Load H2O Ensemble Models\n",
    "\n",
    "To load and use an H2O Ensemble model, be sure to also load the base learners. In this example, we will save an ensemble and its base learners to S3 so that we can load them any time in a new H2O instance. \n",
    "\n",
    "This notebook assumes:\n",
    "1. You have already trained an h2o ensemble model called 'ensemble' that has a model id 'ensemble_id'\n",
    "2. You have a python list of your model ids called 'baseList'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to upload a file to S3\n",
    "\n",
    "def upload_file_to_s3(myFile):\n",
    "    def get_bucket():\n",
    "        access=  # Enter your access key\n",
    "        secret=  # Enter your secret key\n",
    "        conn = S3Connection(access,secret)\n",
    "        b = conn.get_bucket('YOUR-BUCKET-NAME-GOES-HERE',validate=False)  # Enter your bucket name \n",
    "        return b\n",
    "\n",
    "    s3_bucket = get_bucket()\n",
    "    k = Key(s3_bucket)    \n",
    "    k.key = myFile\n",
    "    k.set_contents_from_filename(myFile)\n",
    "    k.make_public()\n",
    "    successMessage = \"Uploaded %s to S3.\"%(myFile)    \n",
    "    return successMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save each of the models used in the ensemble\n",
    "\n",
    "for i in range(0,len(baseList)):                     # Where baseList is the list of model ids used in the ensemble\n",
    "    gridmodel = h2o.get_model(baseList[i])\n",
    "    myFile = h2o.save_model(gridmodel, force=True)   # Save each model\n",
    "    upload_file_to_s3(myFile)                        # Upload each model to s3\n",
    "    print(\"Uploaded \" + str(baseList[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath = os.path.dirname(os.path.abspath(myFile))       # Get the local path where the models are saved. Will be the same on S3.\n",
    "uploadBaseList = pd.DataFrame(baseList)                 # Dataframe to store the list of model ids\n",
    "filename = myPath + '/' + 'uploadBaseList.csv'          # A file that will store the list of model ids on S3\n",
    "uploadBaseList.to_csv(filename, header=None)            # Convert the pandas dataframe to a csv\n",
    "upload_file_to_s3(filename)                             # Upload that csv of model ids to s3\n",
    "print(\"Uploaded \" + str(filename))\n",
    "\n",
    "ensembleModelFile = h2o.save_model(ensemble, force=True)   # Save the h2o ensemble model. If your model is called something different, change 'ensemble' to your model name\n",
    "upload_file_to_s3(ensembleModelFile)                       # Upload the model to S3 \n",
    "print(\"Uploaded \" + str(ensembleModelFile))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_file_from_s3(key):\n",
    "    def get_bucket():            \n",
    "        access=  # Enter your access key\n",
    "        secret=  # Enter your secret key\n",
    "        conn = S3Connection(access,secret)\n",
    "        b = conn.get_bucket('YOUR-BUCKET-NAME-GOES-HERE',validate=False)  # Enter your bucket name \n",
    "        return b\n",
    "\n",
    "    s3_bucket = get_bucket()\n",
    "    payload = s3_bucket.get_key(key)\n",
    "    local_file = payload.get_contents_to_filename(key)\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download remote file containing the list of model ids\n",
    "# Enter your bucket name and modify the aws path as needed\n",
    "\n",
    "baseModelids = pd.read_csv('https://s3-us-west-1.amazonaws.com/YOUR-BUCKET-NAME-GOES-HERE/home/jupyter/uploadBaseList.csv', header=None, delimiter=',')   # Download the list of model ids the pandas dataframe to a csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in baseModelids.index:                       \n",
    "    pull_file_from_s3('/home/jupyter/' + str(baseModelids.iloc[i,1]))\n",
    "    h2o.load_model('/home/jupyter/' + str(baseModelids.iloc[i,1]))\n",
    "    print('Loaded: ' + str(baseModelids.iloc[i,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the model from s3\n",
    "downloaded_model = pull_file_from_s3('/home/jupyter/Predict-Churn/ensemble_id')  \n",
    "\n",
    "# load the downloaded model into memory\n",
    "myEnsmblePredictor = h2o.load_model(path=downloaded_model)\n"
   ]
  }
 ],
 "metadata": {
  "_datascience": {
   "notebookId": 608,
   "post_id": "AVs_WCYcdxOghr0gw_3H"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
